\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage[scaled]{helvet}
\geometry{
    a4paper,
    margin=2.5cm,
    includehead,
    includefoot
}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=black]{hyperref}
\usepackage{cleveref}
\usepackage{booktabs}
\usepackage{bbm}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}

\title{Memory makes computation universal, remember?}
\author{Erik Garrison\\
  \texttt{egarris5@uthsc.edu}\\[1ex]
  }
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
Recent breakthroughs in AI capability have been attributed to increasingly sophisticated architectures and alignment techniques, but a simpler principle may explain these advances: memory makes computation universal.
Memory enables universal computation through two fundamental capabilities: recursive state maintenance and reliable history access.
We formally prove these requirements are both necessary and sufficient for universal computation.
This principle manifests across scales, from cellular computation to neural networks to language models.
Complex behavior emerges not from sophisticated processing units but from maintaining and accessing state across time.
We demonstrate how parallel systems like neural networks achieve universal computation despite limitations in their basic units by maintaining state across iterations.
This theoretical framework reveals a universal pattern: computational advances consistently emerge from enhanced abilities to maintain and access state rather than from more complex basic operations.
Our analysis unifies understanding of computation across biological systems, artificial intelligence, and human cognition, reminding us that humanity's own computational capabilities have evolved in step with our technical ability to remember through oral traditions, writing, and now computing.
\end{abstract}

\section{Introduction}

We are collections of cells with interwoven systems of memory.
The genome maintains computational state through an unbroken chain of survival, while molecular markers preserve cell identity, immune systems learn from encounters, and neural architectures give rise to consciousness.
Through memory, we became beings that could understand our own existence.
Our species' greatest evolutionary leap came not through biological adaptation but through cultural innovation---the ability to maintain and pass on our thoughts beyond individual lifespans through oral traditions, writing, and libraries.
Each technology expanded our ability to preserve and process information across time, transforming human civilization through new forms of memory.
This cultural evolution of memory systems mirrors the fundamental role of memory in all computation, from cellular mechanisms to artificial intelligence.

When we finally mastered computation, we did it by capturing lightning in rock: precise control of electron flows in crystalline structures.
Yet as we strive to create systems that can understand and generate human language, we find ourselves returning to biological patterns: massive parallel networks trained simultaneously across countless processors.
These parallel training constraints lead our artificial systems to mirror our own biological limitations---they too are restricted to threshold computations, like our ``blobby, wet'' molecular mechanisms \cite{alberts2022molecular}.
Their intelligence emerges not from increasingly sophisticated components but from maintaining and accessing state across time, just as ours did.

Memory is not merely a feature of computational systems: it is the key capability that enables universal computation~\cite{turing1936computable}.
Even systems with very basic state transitions can achieve universal computation when equipped with memory~\cite{merrill2023parallelism,peng2024limitations}.
This principle is demonstrated by remarkably minimal universal systems like Rule 110 cellular automata~\cite{cook2004universality} and the universality of the mov+jmp instruction set~\cite{savage1994space}.
We show how this fundamental insight manifests in computational systems, from neural networks to biological cells~\cite{wang2023parallel}, where simple parallel operations combined with state maintenance enable complex computation~\cite{swamy1983space,bisaz2024memory}.

We demonstrate how two basic capabilities---recursive state maintenance and reliable history access---are sufficient for universal computation \cite{savage1994space,bennett1989time}.
We first establish this formally through a constructive proof, grounding our subsequent analysis in computational theory.
We then examine how parallel processing limitations manifest in both artificial neural networks and biological systems, showing how these systems achieve complex computation through memory rather than sophisticated processing units.
Finally, we explore the practical implications for artificial intelligence development, particularly in the context of large language models and recursive reasoning.
This analysis provides a unified perspective on computation across domains while suggesting new directions for AI architecture development focused on robust state maintenance mechanisms.

\section{Memory enables universal computation}

A system has recursive state processing if it can read its current state $s$, generate an update function $f(s)$ that produces the next state, and maintain the new state $f(s)$ for subsequent processing.
This state maintenance must be robust against errors and degradation.

A system has reliable history access if it can: (1) reference any previous state $s_i$ from its computation history without error, (2) distinguish between different states in its history unambiguously, (3) maintain and determine the correct temporal order of states, establishing clear boundaries between computational periods, and (4) guarantee the integrity of stored states.

These capabilities enable universal computation through a straightforward construction \cite{sipser1996introduction,deutsch1995universality,bennett1989time}.
Given a Universal Turing Machine $U$, we implement it as follows: The state $s = (q, p, a)$ encodes the current machine state $q$, the position $p$ of the tape head, and the symbol $a$ under the head.
At each step, the system processes current state $s$ to determine $(q', a', d) = \delta(q, a)$, where $q'$ is the next machine state, $a'$ is the symbol to write, and $d$ is the head movement.
It uses history access to reconstruct tape contents as needed and maintains the new state for the next step.

We show how these two requirements enable universal computation through a constructive proof that proceeds in three steps: encoding the machine state, simulating tape operations, and proving correctness with complexity bounds.

\vspace{1em}

\begin{theorem}[Universality]
Any system $S$ with recursive state maintenance and reliable history access can simulate a Universal Turing Machine with at most logarithmic overhead in space and time complexity \cite{boyle2024memory,liskiewicz1994complexity}.
\end{theorem}

\begin{proof}
We proceed in three steps:

\vspace{0.5em}
\noindent\underline{First}, we show that $S$ can implement the basic operations of a UTM. Let $M$ be a UTM with state set $Q$, tape alphabet $\Gamma$, and transition function $\delta$. We construct a simulation in $S$ as follows:

\vspace{0.5em}
\noindent\textbf{State Encoding:} Each configuration of $M$ is encoded as a tuple $s = (q, p, a, t)$ where: $q \in Q$ is the current machine state, $p \in \mathbb{N}$ is the head position, $a \in \Gamma$ is the symbol under the head, and $t \in \mathbb{N}$ is the step counter.

\vspace{0.5em}
\noindent\textbf{Tape Simulation:} For each position $p$ visited by $M$, we maintain a history entry $h_p = (p, a_p, t_p)$ where $a_p \in \Gamma$ is the symbol written at position $p$ and $t_p$ is the time step when this symbol was written.

\vspace{0.5em}
\noindent\underline{Second}, we prove that these operations preserve computational state correctly through the following invariants:

\begin{lemma}[State Coherence]
At each step $t$, the simulated configuration $(q, p, a)$ exactly matches the configuration of $M$ after $t$ steps on the same input.
\end{lemma}

\begin{proof}
By induction on $t$:

\vspace{0.3em}
\noindent\textit{Base case} ($t=0$): The initial configuration matches by construction.

\vspace{0.3em}
\noindent\textit{Inductive step}: Assume the invariant holds for step $t$. 
For step $t+1$, $S$ reads current state $(q, p, a, t)$, computes $(q', a', d) = \delta(q, a)$, updates position $p' = p + d$, uses history access to find $a''$ at $p'$, and maintains new state $(q', p', a'', t+1)$.
This exactly mirrors $M$'s transition function, preserving the invariant.
\end{proof}

\begin{lemma}[History Consistency]
For any position $p$ and time $t$, the symbol recorded in history matches what would be on $M$'s tape at position $p$ at time $t$.
\end{lemma}

\begin{proof}
We maintain two invariants: each write operation creates a history entry with the current time step, and when reading position $p$, we retrieve the entry with maximum $t_p \leq t$.
This ensures we always see the most recent write to each position, exactly matching $M$'s tape contents.
\end{proof}

\vspace{0.5em}
\noindent\underline{Third, and finally}, we analyze the complexity bounds: The simulation incurs logarithmic overhead in both space ($O(\log t)$ bits for step counter and $O(\log n)$ bits for position encoding) and time ($O(\log t)$ for history access operations). These bounds are tight \cite{parzych2024memory,hhan2024new,boyle2024memory}, as demonstrated through recent impossibility results. Therefore, $S$ simulates $M$ with logarithmic overhead in both space and time \cite{savage1994space,vonkorff2019molecular,bennett1989time}.
\end{proof}

\begin{corollary}[Universality]
Any system with recursive state maintenance and reliable history access is Turing-complete.
\end{corollary}

\vspace{1em}

Consider a basic threshold unit that checks if a number is greater than zero.
Without memory, this unit can only output yes/no signals---a trivial, static operation.
However, with memory to store previous results, this same unit becomes capable of counting: starting from zero, it can repeatedly increment its input and maintain the running total, transforming a simple parallel threshold operation into a sequential counter.
This counter then serves as a fundamental building block for more complex operations, enabling program sequencing and even Turing machine simulation.

Reliable history access requires systems to maintain clear boundaries between computational states---a requirement that manifests as temporal structure in physical implementations.
In neural networks, this structure emerges through attention windows that establish sequence order and layered processing that maintains state boundaries \cite{martini2015information,quentin2019differential}.
Biological systems show similar patterns: cells partition computation through cell cycle phases and stable epigenetic states \cite{bruno2022epigenetic}, while developing organisms maintain state through precisely timed developmental stages and morphology \cite{turing1952chemical}.
Despite their diverse implementations, all these systems share a common need to establish unambiguous boundaries between computational states, enabling reliable access to their processing history.

\section{Memory constraints in parallel systems}

Both artificial and biological systems face fundamental constraints in parallel computation that shape their architecture and capabilities.
These constraints emerge from different sources but lead to remarkably similar solutions through memory and state maintenance.

Recent empirical work provides striking validation of this principle.
Feng et al. \cite{feng2024rnns} demonstrated that RNNs stripped to their minimal state maintenance operations, but reorganized for parallel processing, match the performance of recent sequence models.
This shows how basic state maintenance enables complex computation.

\subsection{Parallel processing limitations}

Neural architectures face a fundamental computational barrier: they are restricted to $\text{TC}_0$ complexity, as proven by recent theoretical work \cite{merrill2023parallelism,peng2024limitations}.
This limitation stems directly from the requirements of parallel training at scale.
To train efficiently across multiple devices, neural networks must permit independent parameter updates across different portions of the network \cite{barrett2019analyzing}.
This independence requirement manifests in different ways: transformers treat each position independently modulo attention weights, convolutional networks rely on spatial locality, and even recurrent networks must make independence assumptions between time steps when unrolled for parallel training \cite{dickson2023rnns}.
Even recent architectural innovations like structured state spaces and their variants reformulate sequential computation for parallel training while retaining the core limitations of recurrent processing \cite{gu2021efficiently,gu2023mamba,dao2024transformers,nguyen2024hyenadna}.

Similarly, biological systems face parallel processing constraints due to their physical nature.
The central dogma of molecular biology reveals how cells solve computation through inherently parallel mechanisms \cite{wang2023parallel,cai2024efficient,fu2023scgrn}.
Unable to engineer precise electromagnetic flows like silicon systems, cells must process information through vast networks of simultaneous molecular interactions.
Every aspect of cellular computation occurs through parallel reactions governed by concentration thresholds and binding affinities \cite{alberts2022molecular}.
A typical eukaryotic cell contains gigabase-pairs of DNA, comparable RNA, and billions of proteins \cite{milo2013protein} tumbling at gigahertz frequencies \cite{zhang2023molecular}, vibrating in the megahertz to terahertz range, and diffusing across the cell in seconds.
The physical impossibility of imposing sequential order on billions of molecules in constant motion forces parallel computation, just as training efficiency forces parallelism in artificial neural networks.

\subsection{Overcoming limitations through state maintenance}

Both artificial and biological systems overcome these parallel constraints through sophisticated state maintenance mechanisms.
In neural networks, each additional processing step increases computational capability in a measurable way, but only when proper state maintenance is preserved across steps.
Recent work shows this directly supports our framework by demonstrating how memory mechanisms, not architectural complexity, determine computational power \cite{merrill2024}.

The brain demonstrates similar principles through precise architectural design.
In the hippocampal-entorhinal system, dedicated ``time cells'' fire in precise sequences while ramping cells modulate their activity to provide temporal context \cite{quentin2019differential}.
The prefrontal cortex coordinates with the hippocampus to organize memories, while the parahippocampal region processes spatial and event details \cite{martini2015information}.
This system achieves ordered episodic memories not by making individual neurons sequential processors, but by maintaining state through multiple parallel components orchestrated in time.

\subsection{Evolutionary solutions for state maintenance}

Biological systems have evolved three primary strategies for maintaining computational state, each demonstrating the fundamental role of memory in enabling complex computation.

The CRISPR-Cas system represents perhaps the most elegant implementation of biological memory.
This remarkably efficient system maintains a chronological record of viral infections \cite{kim2019crispr}.
By separating stable DNA-based storage from active protein-based processing \cite{sadremomtaz2023digital}, CRISPR demonstrates how evolution converged on fundamental solutions for maintaining computational state.
The system achieves this through a minimal yet effective architecture that combines reliable state storage with precise processing mechanisms.

Neural systems implement a second key strategy through sophisticated memory structures that preserve both state and temporal relationships.
In the hippocampal-entorhinal system, dedicated time cells fire in precise sequences while ramping cells modulate their activity to provide temporal context \cite{quentin2019differential}.
The prefrontal cortex coordinates with the hippocampus to organize memories, while the parahippocampal region processes spatial and event details \cite{martini2015information}.
This system achieves ordered episodic memories not by making individual neurons sequential processors, but by maintaining state through multiple parallel components orchestrated in time.

Developmental biology provides the third major paradigm through pattern formation and morphogenesis.
The anterior-posterior axis and segmentation patterns in animal embryos emerge through maintained morphogen gradients and sequential state updates \cite{pastor2020computation}.
These patterns demonstrate remarkable robustness, with the ability to regenerate proper proportions even after significant tissue removal or reorganization \cite{lobo2012towards}.
This principle was first formalized by Turing's reaction-diffusion theory \cite{turing1952chemical}, showing how simple molecular interactions can implement sophisticated state maintenance through physical structure.
Through the interaction of fast-diffusing inhibitors with self-enhancing activators, organisms create stable spatial patterns that maintain developmental state information across time.

These three systems share a common principle: they achieve complex computation not through sophisticated processing units, but through reliable state maintenance mechanisms.
Recent work in synthetic biology validates this framework, successfully implementing recording systems that combine DNA-based storage with protein-based processing \cite{sheth2017multiplex}.
At the cellular level, even apparently sequential processes like the cell cycle are implemented through parallel molecular networks with threshold-based transitions rather than true sequential computation \cite{alberts2022molecular}.
This mirrors how artificial systems overcome parallel processing limitations through recursive state maintenance rather than more complex basic units.

\section{Practical implications}

The lens of recursive state maintenance reveals fundamental patterns in how language models achieve complex reasoning \cite{dickson2024trust,ahn2024recursive,openai2024o1}.
These systems face inherent parallel processing limitations that constrain their computational capabilities \cite{merrill2023parallelism}.
Recent evidence demonstrates both the power and limitations of chain-of-thought approaches in overcoming these constraints \cite{liu2024mind}.
Traditional scaling approaches focused on increasing parallel processing power through larger models and more sophisticated attention mechanisms \cite{shallue2019measuring}.
However, our analysis shows this approach alone cannot overcome the fundamental limitations in sequential reasoning \cite{peng2024limitations}.
Language models achieve sequential reasoning by learning to replicate computational patterns embedded in human language.
Through exposure to text containing human reasoning traces, these models internalize the basic transition functions that characterize structured thought.
When provided with sufficient context as state, they can apply these learned patterns to perform step-by-step reasoning.
This explains why chain-of-thought prompting proves effective: it provides a framework for maintaining computational state across multiple steps.

Recent empirical evidence from the ARC Prize competition demonstrates the limitations of pure scaling approaches.
While modern LLMs with sophisticated architectures achieve only 5-14\% accuracy through direct prompting, the introduction of state maintenance mechanisms through test-time training improved performance to 55.5\% \cite{chollet2024arc}.
This dramatic improvement aligns with our framework's prediction: parallel processing capabilities alone cannot overcome sequential reasoning limitations without proper state maintenance.
Just as biological systems don't solve sequential computation by making cells more complex \cite{wang2023parallel}, we won't solve it by making transformer layers more sophisticated \cite{zhao2024epha}.

Recent results from OpenAI's o3 model provide further evidence for this view.
The system achieves 82.8\% accuracy on 400 public tasks in high-efficiency mode (requiring \$6,677, using approximately 33M tokens) and 91.5\% in low-efficiency mode (using approximately 9.5B tokens, at an estimated cost of \$1.15M) \cite{chollet2024o3}.
Our framework suggests that what's o3's authors frame as ``deliberative alignment'' \cite{openai2024o3blog,guan2024deliberative} may be better understood as an emergent property of achieving reliable computation through memory and state maintenance, building on earlier work with the o1 model \cite{openai2024learning}.
Since a system without reliable state maintenance cannot consistently execute any computational pattern---including following policies---the o3 system compensates by searching over potential computational paths, using policy adherence as a way to detect which paths successfully maintain coherent computational state \cite{chollet2024o3, openai2024learning}.
In other words, reliable policy following and reliable computation are fundamentally inseparable: a system capable of one must be capable of the other.
The substantial computational costs---\$17-20 per task in high-efficiency mode---likely reflect both the quadratic overhead of transformer attention and the need to evaluate multiple possible computational traces.
This suggests o3 represents something qualitatively different from traditional LLMs \cite{willison2024o3}---by achieving reliable state maintenance through parallel search and selection, albeit at significant cost, it appears to meet the requirements for universal computation outlined in our framework.
The system's impressive performance may emerge not from sophisticated ethical reasoning, but from its ability to maintain consistent computational state while searching for valid solutions that follow learned patterns.

Instead, architectural innovation should focus on robust mechanisms for state maintenance and recursive processing \cite{jung2020new}.
This might involve memory systems that maintain coherent state across multiple processing steps \cite{zhu2024overcoming}.
Recent research has identified specific conditions where chain-of-thought reasoning can actually reduce performance, particularly in tasks where the underlying computation requires implicit pattern recognition rather than explicit reasoning \cite{liu2024mind}.
This aligns with our framework's prediction that memory-based recursive computation is most effective when the task requires true sequential dependence, rather than parallel pattern matching.
State verification mechanisms that ensure consistency across recursive processing steps, similar to how biological systems maintain state coherence through specialized cellular structures \cite{espinosa2024molecular}, become crucial for maintaining reliable computation.


This framework reveals distinct modes of computation in language models \cite{wei2022chain}.
Pattern matching operates through parallel processing, enabling rapid recognition through direct feature detection.
Sequential reasoning builds on this by maintaining state across steps, enabling complex deductions through recursive composition \cite{dickson2024trust}.
This distinction explains why chain-of-thought reasoning can sometimes reduce performance, particularly for tasks better suited to implicit pattern recognition \cite{liu2024mind}.
The effect mirrors human cognition, where explicit verbal reasoning can interfere with intuitive pattern matching.

The relationship between compression and computation provides further insight.
Language models compress human knowledge into distributed representations of text patterns.
This compression differs fundamentally from state maintenance \cite{dickson2024trust}.
Compression captures static patterns, while state maintenance enables dynamic computation through recursive updates.
This explains why models can recognize patterns instantly but need explicit prompting for step-by-step reasoning.

Sequence length reveals another key principle.
Short sequences often resolve through pure pattern matching in a single forward pass.
Longer sequences requiring true sequential reasoning demand explicit state maintenance across multiple steps \cite{wei2022chain}.
This creates testable predictions about when recursive reasoning helps or hinders: sequential tasks benefit from chain-of-thought approaches, while pattern-matching tasks may suffer from forced sequential processing \cite{liu2024mind}.

The power of this approach comes from computational history access \cite{fu2024memory}.
Each step in the chain builds on previous results while maintaining clear temporal structure \cite{wei2022chain}.

The remarkable leap in language model capabilities emerged from massive overtraining on an unprecedented coverage of human knowledge \cite{schuurmans2024autoregressive}.
This represents a form of transfer learning where human computational patterns are distilled into the model's parameters.
This extensive exposure enabled these systems to learn and internalize the fundamental transition functions that humans use with language.
When provided with sufficient context (state), these models can achieve human-level performance precisely because they have learned to replicate human computational patterns through exposure to vast collections of text---our recorded explanations, arguments, derivations, and other traces of human thought \cite{brown2020language,wei2022chain}.

When presented with prompts containing examples, LLMs can effectively apply learned patterns to break down complex problems into steps, implementing a form of recursive reasoning \cite{wei2022chain}.
This reasoning occurs without weight updates, suggesting the models have internalized generalizable computational patterns from their training data.
The success of techniques like chain-of-thought prompting demonstrates that these models can effectively replicate human-like problem-solving approaches when given appropriate frameworks for maintaining computational state \cite{wei2022emergent}.

This perspective illuminates why techniques like chain-of-thought prompting prove effective: they provide a framework for applying learned human computational patterns recursively while maintaining state across steps \cite{wei2022chain}.
The success of these methods depends not on teaching models new reasoning capabilities, but on structuring computation to leverage already-learned transition functions in a way that enables reliable state maintenance and access.


\section{Conclusion}

Our analysis reveals a fundamental pattern across scales: from molecular mechanisms to neural systems to artificial intelligence, advances in computational capability emerge from enhanced abilities to maintain and access state---just as human civilization itself advanced through increasingly sophisticated memory technologies.
The recent breakthroughs in language models demonstrate this principle---their power comes not from architectural complexity but from learning human computational patterns while maintaining sufficient context for recursive processing.
The recent emergence of systems like o3 \cite{guan2024deliberative} demonstrates this principle.
The model we present here suggests their capabilities arise not from architectural sophistication but from meeting the fundamental requirements for memory-enabled universal computation, even at substantial computational cost in current implementations.
This suggests a shift in focus for AI development: rather than pursuing larger models, progress requires attention to the memory mechanisms that enable recursive computation.
Evolution's elegant solutions, from molecular switches to neural architectures \cite{burrill2010making}, point toward fundamentally new approaches to reliable computation based on robust state maintenance rather than complex processing units.

\section*{Acknowledgments}
This work was supported by National Science Foundation PPoSS Award \#2118709, National Institutes of Health 1U01HG013760-01, and the University of Tennessee Center for Integrative and Translational Genomics.

\begingroup
\footnotesize
\bibliographystyle{unsrt}
\bibliography{refs}
\endgroup

\end{document}
